{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let's implement loss function calculation.  \n",
    "In this task we'll use loss known as Log-Loss.\n",
    "\n",
    "$$\n",
    "L(\\textbf{w}, \\textbf{X}, \\textbf{y}) = \\sum_{i=1}^{N}log(1 + exp(-y_i x_i^T \\textbf{w})) + \\lambda_1 \\sum_{j=1}^M |w_j| + \\lambda_2 \\sum_{j=1}^M w_j^2\n",
    "$$\n",
    "\n",
    "Here,   \n",
    "$\\textbf{X}$ is a design matrix, $\\textbf{X} \\in \\mathbb{R}^{NxM}$  \n",
    "$\\textbf{x_i}$ is ith training example, $\\textbf{x_i} \\in \\mathbb{R}^M$  \n",
    "$\\textbf{w}$ is weights vector to be optimized, $\\textbf{w} \\in \\mathbb{R}^M$  \n",
    "$y_i$ is the class of ith training example, $y_i \\in \\{-1, 1\\}$  \n",
    "\n",
    "Therefore we can rewrite it in matrix form:  \n",
    "$$\n",
    "L(\\textbf{w}, \\textbf{X}, \\textbf{y}) = log(1 + exp(-(\\textbf{X} \\textbf{w})^T \\textbf{y})) + \\lambda_1 \\| \\textbf{w} \\|_1 + \\lambda_2 \\| \\textbf{w} \\|_2\n",
    "$$  \n",
    "\n",
    "Here,   \n",
    "$\\|\\textbf{a}\\|_1$ is L1 norm  \n",
    "$\\|\\textbf{a}\\|_2$ is L2 norm  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lossf(w, X, y, l1, l2):\n",
    "    \"\"\"\n",
    "    :param w: numpy.array of size (M,) dtype = np.float\n",
    "    :param X: numpy.array of size (N, M), dtype = np.float\n",
    "    :param y: numpy.array of size (N,), dtype = np.int\n",
    "    :param l1: float, l1 regularizer coefficient\n",
    "    :param l2: float, l2 regularizer coefficient \n",
    "    :return: float, value of loss function\n",
    "    \"\"\"\n",
    "    lossf = np.log(1 + np.exp(-np.matmul(np.matmul(X, w).T, y))) + \\\n",
    "                   l1 * np.linalg.norm(w, ord=1) + \\\n",
    "                   l2 * np.square(np.linalg.norm(w, ord=2))\n",
    "    return lossf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we should implement gradient calculation.  \n",
    "Gradient is a vector containing partial derivatives of a function\n",
    "$$\n",
    "\\nabla F (x, y, z) = (\\frac{\\partial F}{\\partial x}, \\frac{\\partial F}{\\partial y}, \\frac{\\partial F}{\\partial z})^T\n",
    "$$\n",
    "\n",
    "Therefore, we need to compute partial derivatives for target function.\n",
    "$$\n",
    "\\frac{\\partial L(\\textbf{w}, \\textbf{X}, \\textbf{y})}{\\partial w_j} = -\\frac{exp(-(\\textbf{X} \\textbf{w})^T \\textbf{y})}{1 + exp(-(\\textbf{X} \\textbf{w})^T \\textbf{y}))} * \\pmb{x}^{jT} \\textbf{y} + \\lambda_1 sgn (w_j) + 2 \\lambda_2 w_j\n",
    "$$\n",
    "\n",
    "Here,  \n",
    "$sgn(x)$ is signum function  \n",
    "$x^j$ is vector containing values of feature j  \n",
    "\n",
    "Rewrite it into matrix form:  \n",
    "$$\n",
    "\\frac{\\partial L(\\textbf{w}, \\textbf{X}, \\textbf{y})}{\\partial \\textbf{w}} = -\\frac{exp(-(\\textbf{X} \\textbf{w})^T \\textbf{y})}{1 + exp(-(\\textbf{X} \\textbf{w})^T \\textbf{y}))} * \\textbf{X}^T \\textbf{y}  + \\lambda_1 sgn (\\textbf{w}) + 2 \\lambda_2 \\textbf{w} = (\\frac{1}{1 + exp(-(\\textbf{X} \\textbf{w})^T \\textbf{y}))} - 1) * \\textbf{X}^T \\textbf{y}  + \\lambda_1 sgn (\\textbf{w}) + 2 \\lambda_2 \\textbf{w}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gradf(w, X, y, l1, l2):\n",
    "    \"\"\"\n",
    "    :param w: numpy.array of size (M,), dtype = np.float\n",
    "    :param X: numpy.array of size (N, M), dtype = np.float\n",
    "    :param y: numpy.array of size (N,), dtype = np.int\n",
    "    :param l1: float, l1 regularizer coefficient \n",
    "    :param l2: float, l2 regularizer coefficient \n",
    "    :return: numpy.array of isze (M,), dtype = np.float, gradient vector d lossf / dw\n",
    "    \"\"\"\n",
    "    p = np.matmul(np.matmul(X, w).T, y)\n",
    "    gradw = (special.expit(p) - 1) * np.matmul(X.T, y) + l1 * np.sign(w) + 2 * l2 * w\n",
    "    return gradw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to implement learning with gradient descent method.  \n",
    "The main idea here is to iteratively move towards the solution.\n",
    "\n",
    "$$\n",
    "\\textbf{w} (t = n+1) = \\textbf{w} (t = n) - \\eta \\nabla(\\textbf{w})\n",
    "$$\n",
    "\n",
    "Here,\n",
    "$\\eta$ is a learning rate, variable determining speed of fitting weights.  \n",
    "\n",
    "Also we need a function to predict class of new examples.  \n",
    "Clearly, dot product of $\\textbf{x_i}$ and $\\textbf{w}$ is unbounded, so we need to squeeze our predictions to the range $[0, 1]$  \n",
    "The most common choice for that is the sigmoid function:  \n",
    "$\\sigma(x) = \\frac{1}{1 + exp(-x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start test\n",
      "Accuracy: 0.903\n",
      "End tests\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# Используйте scipy.special для вычисления численно неустойчивых функций\n",
    "# https://docs.scipy.org/doc/scipy/reference/special.html#module-scipy.special\n",
    "\n",
    "\n",
    "class LR(ClassifierMixin, BaseEstimator):\n",
    "    def __init__(self, lr=1, l1=1e-4, l2=1e-4, num_iter=1000, verbose=0):\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "        self.w = None\n",
    "        self.lr = lr\n",
    "        self.verbose = verbose\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Logistic regression training.\n",
    "        self.w is to be fitted here.\n",
    "\n",
    "        self.verbose == True means output of loss values for every iteration\n",
    "\n",
    "        :param X: numpy.array of size (N, M), dtype = np.float\n",
    "        :param y: numpy.array of size (N,), dtype = np.int\n",
    "        :return: self\n",
    "        \"\"\"\n",
    "        \n",
    "        n, d = X.shape\n",
    "        self.w = np.random.randn(d)\n",
    "        self.w[self.w < -3] = -3\n",
    "        self.w[self.w > 3] = 3\n",
    "\n",
    "        for i in range(self.num_iter):\n",
    "            w_new = self.w - self.lr * gradf(self.w, X, y, self.l1, self.l2)\n",
    "                \n",
    "            if self.verbose != 0:\n",
    "                print \"Iteration \" + str(i) + \". Current loss is \" + str(lossf(self.w, X, y, self.l1, self.l2))\n",
    "            self.w = w_new\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Predict probability of belonging objects to class 1.\n",
    "\n",
    "        :param X: numpy.array of size (N, M), dtype = np.float\n",
    "        :return: numpy.array of size (N,), dtype = np.int\n",
    "        \"\"\"\n",
    "        return special.expit(np.matmul(X, self.w))\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Class prediction\n",
    "        Returns np.array of size(N,) of 1 and -1.\n",
    "\n",
    "        :param X: numpy.array of size (N, M), dtype = np.float\n",
    "        :return:  numpy.array of size (N,), dtype = np.int\n",
    "        \"\"\"\n",
    "        predicts = (self.predict_proba(X) > .5) * 2 - 1\n",
    "        return predicts \n",
    "\n",
    "\n",
    "def test_work():\n",
    "    X, y = make_classification(n_features=100, n_samples=1000)\n",
    "    print \"Start test\"\n",
    "\n",
    "    y = 2 * (y - 0.5)\n",
    "\n",
    "    try:\n",
    "        clf = LR(lr=1, l1=1e-4, l2=1e-4, num_iter=10000, verbose=0)\n",
    "    except Exception:\n",
    "        assert False, \"Unable to create the model\"\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        clf = clf.fit(X, y)\n",
    "    except Exception:\n",
    "        assert False, \"Unable to fit the model\"\n",
    "        return\n",
    "\n",
    "    print \"Accuracy: \" + str((clf.predict(X) == y).mean())\n",
    "\n",
    "    assert isinstance(lossf(clf.w, X, y, 1e-3, 1e-3), float), \"The loss function returns non-scalar value\"\n",
    "    assert gradf(clf.w, X, y, 1e-3, 1e-3).shape == (100,), \"Wrong size of grad\"\n",
    "    assert gradf(clf.w, X, y, 1e-3, 1e-3).dtype == np.float, \"Wrong type of grad\"\n",
    "    assert clf.predict(X).shape == (1000,), \"Wrong size of prediction vector\"\n",
    "    assert np.min(clf.predict_proba(X)) >= 0, \"Probabilities are less than 0\"\n",
    "    assert np.max(clf.predict_proba(X)) <= 1, \"Probabilities are greater than 1\"\n",
    "    assert len(set(clf.predict(X))) == 2, \"There are more than 2 classes!\"\n",
    "    print \"End tests\"\n",
    "\n",
    "test_work()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
